import feedparser
import requests
import json
import time
from datetime import datetime
import os
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class ArticleCrawler:
    def __init__(self, data_dir='article_data'):
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)
    
    def crawl_vnexpress_rss(self, max_articles=10000, verbose=False):
        print("ƒêANG CRAWL B√ÄI B√ÅO T·ª™ C√ÅC TRANG B√ÅO...")
        print("=" * 50)
        
        rss_feeds = [
            # === B√ÅO VI·ªÜT NAM (HO·∫†T ƒê·ªòNG T·ªêT) ===
            'https://vnexpress.net/rss/tin-moi-nhat.rss',
            'https://vnexpress.net/rss/thoi-su.rss',
            'https://vnexpress.net/rss/the-gioi.rss',
            'https://vnexpress.net/rss/kinh-doanh.rss',
            'https://vnexpress.net/rss/giai-tri.rss',
            'https://vnexpress.net/rss/the-thao.rss',
            'https://vnexpress.net/rss/phap-luat.rss',
            'https://vnexpress.net/rss/giao-duc.rss',
            'https://vnexpress.net/rss/suc-khoe.rss',
            'https://vnexpress.net/rss/doi-song.rss',
            'https://vnexpress.net/rss/du-lich.rss',
            'https://vnexpress.net/rss/khoa-hoc.rss',
            'https://vnexpress.net/rss/so-hoa.rss',
            'https://vnexpress.net/rss/oto-xe-may.rss',
            
            'https://dantri.com.vn/rss/thoi-su.rss',
            'https://dantri.com.vn/rss/the-gioi.rss',
            'https://dantri.com.vn/rss/kinh-doanh.rss',
            'https://dantri.com.vn/rss/giai-tri.rss',
            'https://dantri.com.vn/rss/the-thao.rss',
            'https://dantri.com.vn/rss/giao-duc.rss',
            'https://dantri.com.vn/rss/suc-khoe.rss',
            'https://dantri.com.vn/rss/du-lich.rss',
            
            'https://tuoitre.vn/rss/thoi-su.rss',
            'https://tuoitre.vn/rss/the-gioi.rss',
            'https://tuoitre.vn/rss/kinh-doanh.rss',
            'https://tuoitre.vn/rss/giai-tri.rss',
            'https://tuoitre.vn/rss/the-thao.rss',
            'https://tuoitre.vn/rss/giao-duc.rss',
            'https://tuoitre.vn/rss/suc-khoe.rss',
            
            'https://thanhnien.vn/rss/thoi-su.rss',
            'https://thanhnien.vn/rss/the-gioi.rss',
            'https://thanhnien.vn/rss/kinh-doanh.rss',
            'https://thanhnien.vn/rss/giai-tri.rss',
            'https://thanhnien.vn/rss/the-thao.rss',
            'https://thanhnien.vn/rss/giao-duc.rss',
            'https://thanhnien.vn/rss/suc-khoe.rss',
            
            'https://vietnamnet.vn/rss/thoi-su.rss',
            'https://vietnamnet.vn/rss/the-gioi.rss',
            'https://vietnamnet.vn/rss/kinh-doanh.rss',
            'https://vietnamnet.vn/rss/giai-tri.rss',
            'https://vietnamnet.vn/rss/the-thao.rss',
            'https://vietnamnet.vn/rss/giao-duc.rss',
            'https://vietnamnet.vn/rss/suc-khoe.rss',
            
            'https://zingnews.vn/rss/thoi-su.rss',
            'https://zingnews.vn/rss/the-gioi.rss',
            'https://zingnews.vn/rss/kinh-doanh.rss',
            'https://zingnews.vn/rss/giai-tri.rss',
            'https://zingnews.vn/rss/the-thao.rss',
            
            'https://laodong.vn/rss/thoi-su.rss',
            'https://laodong.vn/rss/the-gioi.rss',
            'https://laodong.vn/rss/kinh-doanh.rss',
            'https://laodong.vn/rss/the-thao.rss',
            
            'https://cafef.vn/rss/thoi-su.rss',
            'https://cafef.vn/rss/the-gioi.rss',
            'https://cafef.vn/rss/kinh-doanh.rss',
            'https://cafef.vn/rss/thi-truong.rss',
            
            'https://vtv.vn/rss/thoi-su.rss',
            'https://vtv.vn/rss/the-gioi.rss',
            'https://vtv.vn/rss/kinh-te.rss',
            
            # === B√ÅO TH·ªÇ THAO VI·ªÜT NAM ===
            'https://www.24h.com.vn/rss/tin-bong-da.rss',
            'https://bongdaplus.vn/rss/bong-da-viet-nam.rss',
            'https://bongdaplus.vn/rss/bong-da-quoc-te.rss',
            'https://thethao247.vn/rss/tin-bong-da.rss',
            'https://webthethao.vn/rss/bong-da.rss',
            
            # === B√ÅO QU·ªêC T·∫æ HO·∫†T ƒê·ªòNG T·ªêT ===
            'https://feeds.bbci.co.uk/news/rss.xml',
            'https://feeds.bbci.co.uk/news/world/rss.xml',
            'https://feeds.bbci.co.uk/news/business.rss.xml',
            'https://feeds.bbci.co.uk/news/technology.rss.xml',
            'https://feeds.bbci.co.uk/news/science_and_environment.rss.xml',
            
            'https://www.theguardian.com/international/rss',
            'https://www.theguardian.com/world.rss',
            'https://www.theguardian.com/business.rss',
            'https://www.theguardian.com/technology.rss',
            'https://www.theguardian.com/science.rss',
            
            'https://apnews.com/apf-topnews?format=xml',
            'https://apnews.com/apf-worldnews?format=xml',
            'https://apnews.com/apf-business?format=xml',
            'https://apnews.com/apf-technology?format=xml',
            
            # === TH·ªÇ THAO QU·ªêC T·∫æ HO·∫†T ƒê·ªòNG T·ªêT ===
            'https://www.espn.com/espn/rss/soccer/news',
            'https://www.goal.com/feeds/en/news',
            'https://www.espn.com/espn/rss/nba/news',
            'https://www.espn.com/espn/rss/nfl/news', 
            'https://www.espn.com/espn/rss/mlb/news',
            'https://www.espn.com/espn/rss/nhl/news',
            'https://www.espn.com/espn/rss/tennis/news',
            'https://www.espn.com/espn/rss/golf/news',
            'https://www.espn.com/espn/rss/racing/news',

            'https://www.skysports.com/rss/12040',  # Football
            'https://www.skysports.com/rss/12036',  # Cricket
            'https://www.skysports.com/rss/12148',  # F1
            'https://www.skysports.com/rss/12150',  # Rugby
            'https://www.skysports.com/rss/12158',  # Golf
            'https://www.skysports.com/rss/12154',  # Tennis
            'https://www.skysports.com/rss/12156',  # Boxing
            'https://www.eurosport.com/rss.xml',

            # === TH·ªÇ THAO CHUY√äN NG√ÄNH ===
            'https://www.atptour.com/en/-/media/rss-feeds/feed-news.aspx',  # Tennis
            'https://www.wtatennis.com/rss/news',  # Tennis n·ªØ
            'https://www.pgatour.com/rss/news.rss',  # Golf
            'https://www.formula1.com/content/fom-website/en/latest/all.rss',  # F1
            'https://www.fiba.basketball/rss',  # Basketball
            'https://www.icc-cricket.com/rss/news',  # Cricket
            
            # === C√îNG NGH·ªÜ & KHOA H·ªåC HO·∫†T ƒê·ªòNG T·ªêT ===
            'https://techcrunch.com/feed/',
            'https://www.theverge.com/rss/index.xml',
            'https://www.wired.com/feed/rss',
            'https://feeds.arstechnica.com/arstechnica/index',
            'https://www.nature.com/subjects/news.rss',

            # === B√ìNG ƒê√Å QU·ªêC T·∫æ ===
            'https://www.premierleague.com/rss',
            'https://www.laliga.com/rss',
            'https://www.bundesliga.com/rss',
            'https://www.legaseriea.it/en/rss',
            'https://www.ligue1.com/rss',

            # === WEBSITE B√ìNG ƒê√Å ===
            'https://www.transfermarkt.com/rss/news',
            'https://www.90min.com/feeds/rss',
            'https://www.fourfourtwo.com/news.rss',
            'https://www.squawka.com/news/feed/',
            'https://www.planetfootball.com/feed/',
            'https://www.football365.com/rss',

            # === NEWSPAPERS ANH ===
            'https://www.dailymail.co.uk/sport/football/index.rss',
            'https://www.mirror.co.uk/sport/football/rss.xml',
            'https://www.thesun.co.uk/sport/football/feed/',
            'https://www.independent.co.uk/sport/football/rss',

            # === TIN CHUY·ªÇN NH∆Ø·ª¢NG ===
            'https://www.football-italia.net/feed',
            'https://www.getfootballnewsgermany.com/feed/',
            'https://www.football-espana.net/feed',

            # === KHOA H·ªåC & THI√äN NHI√äN HO·∫†T ƒê·ªòNG T·ªêT ===
            'https://feeds.bbci.co.uk/news/science_and_environment.rss.xml',
            'https://www.theguardian.com/science.rss',
            'https://apnews.com/apf-science?format=xml',
            'https://www.earthtouchnews.com/feed/',
            'https://www.worldwildlife.org/rss',
            'https://www.nationalgeographic.com/rss/animals',
            'https://www.science.org/rss/news_current.xml',
            'https://www.newscientist.com/section/news/feed/',
            'https://phys.org/rss-feed/',
            'https://www.conservation.org/rss',
            'https://www.greenpeace.org/international/rss/',
            'https://www.space.com/feeds/all',
            'https://www.nasa.gov/rss/dyn/breaking_news.rss',
            'https://www.esa.int/rssfeed/Our_Activities',
            'https://www.skyandtelescope.com/feed/',
            'https://www.sciencedaily.com/rss/top/science.xml',
            'https://www.livescience.com/feeds/all',
            'https://www.sciencenews.org/feed',
            'https://www.discovermagazine.com/rss'
        ]
        
        all_articles = []
        seen_links = set()

        print(f"B·∫Øt ƒë·∫ßu crawl t·ª´ {len(rss_feeds)} ngu·ªìn RSS HO·∫†T ƒê·ªòNG...")

        # TƒÇNG GI·ªöI H·∫†N M·ªñI FEED
        category_limits = {
            'Tin m·ªõi nh·∫•t': 200,
            'Th·ªùi s·ª±': 150, 
            'Th·∫ø gi·ªõi': 150,
            'Kinh doanh': 100,
            'Th·ªÉ thao': 150,
            'B√≥ng ƒë√°': 120,
            'B√≥ng ƒë√° qu·ªëc t·∫ø': 120,
            'B√≥ng ƒë√° Vi·ªát Nam': 100,
            'Gi·∫£i tr√≠': 100,
            'C√¥ng ngh·ªá': 100,
            'S·ª©c kh·ªèe': 100,
            'Gi√°o d·ª•c': 100,
            'ƒê·ªùi s·ªëng': 100,
            'Du l·ªãch': 80,
            'Ph√°p lu·∫≠t': 60,
            'Khoa h·ªçc': 80,
            'Xe': 80,
            'S·ªë h√≥a': 80,
            'International News': 150,
            'World News': 150, 
            'Business News': 120,
            'Technology News': 100,
            'Science News': 100,
            'Sports': 150,
            'Soccer': 150,
        }
        
        default_limit = 100

        # T·∫†O SESSION V·ªöI RETRY
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        session.mount("http://", HTTPAdapter(max_retries=retry_strategy))
        session.mount("https://", HTTPAdapter(max_retries=retry_strategy))
        
        # CRAWL T·ª™NG FEED
        successful_feeds = 0
        failed_feeds = 0
        
        for i, feed_url in enumerate(rss_feeds):
            if len(all_articles) >= max_articles:
                print(f"‚úÖ ƒê√É ƒê·∫†T GI·ªöI H·∫†N {max_articles} B√ÄI B√ÅO!")
                break

            category = self._extract_category(feed_url)
            language = self._extract_language(feed_url)
            limit = category_limits.get(category, default_limit)
            
            if i % 10 == 0:
                print(f"[{i+1}/{len(rss_feeds)}] ƒêang crawl {category}... ({len(all_articles)}/{max_articles})")

            try:
                response = session.get(feed_url, timeout=15)
                feed = feedparser.parse(response.content)
                articles_from_feed = 0
                
                if not feed.entries:
                    print(f"  ‚ö†Ô∏è Feed tr·ªëng: {feed_url}")
                    failed_feeds += 1
                    continue
                
                for entry in feed.entries:
                    if len(all_articles) >= max_articles or articles_from_feed >= limit:
                        break
                    
                    if entry.link in seen_links:
                        continue
                    
                    seen_links.add(entry.link)
                    
                    article = {
                        'id': len(all_articles),
                        'title': entry.title,
                        'summary': entry.summary if hasattr(entry, 'summary') else '',
                        'link': entry.link,
                        'published': entry.published if hasattr(entry, 'published') else '',
                        'category': category,
                        'language': language,
                        'source': self._extract_source(feed_url),
                        'crawled_time': datetime.now().isoformat()
                    }
                    
                    all_articles.append(article)
                    articles_from_feed += 1
                    
                    time.sleep(0.001)
                
                successful_feeds += 1
                print(f"  ‚úÖ {category}: +{articles_from_feed} b√†i")
                            
            except Exception as e:
                failed_feeds += 1
                print(f"  ‚ùå L·ªói v·ªõi feed {feed_url}: {str(e)[:100]}...")
                continue
        
        print(f"\nüéØ HO√ÄN TH√ÄNH CRAWL!")
        print(f"   ‚úÖ Feeds th√†nh c√¥ng: {successful_feeds}")
        print(f"   ‚ùå Feeds th·∫•t b·∫°i: {failed_feeds}")
        print(f"   üìö T·ªïng b√†i b√°o: {len(all_articles)}")
        
        return all_articles

    def _extract_category(self, feed_url):
        categories = {
            # TI·∫æNG VI·ªÜT
            'tin-moi-nhat': 'Tin m·ªõi nh·∫•t',
            'thoi-su': 'Th·ªùi s·ª±', 
            'the-gioi': 'Th·∫ø gi·ªõi',
            'kinh-doanh': 'Kinh doanh',
            'giai-tri': 'Gi·∫£i tr√≠',
            'phap-luat': 'Ph√°p lu·∫≠t',
            'giao-duc': 'Gi√°o d·ª•c',
            'suc-khoe': 'S·ª©c kh·ªèe',
            'doi-song': 'ƒê·ªùi s·ªëng',
            'du-lich': 'Du l·ªãch',
            'khoa-hoc': 'Khoa h·ªçc',
            'so-hoa': 'S·ªë h√≥a',
            'oto-xe-may': 'Xe',
            'the-thao': 'Th·ªÉ thao',
            'cong-nghe': 'C√¥ng ngh·ªá',
            'xa-hoi': 'X√£ h·ªôi',
            'kinh-te': 'Kinh t·∫ø',
            'thi-truong': 'Th·ªã tr∆∞·ªùng',
            
            # B√ìNG ƒê√Å VI·ªÜT NAM
            'bong-da': 'B√≥ng ƒë√°',
            'bong-da-viet-nam': 'B√≥ng ƒë√° Vi·ªát Nam',
            'bong-da-quoc-te': 'B√≥ng ƒë√° qu·ªëc t·∫ø',
            'tin-bong-da': 'B√≥ng ƒë√°',
            
            # B√ìNG ƒê√Å QU·ªêC T·∫æ - GI·∫¢I ƒê·∫§U
            'premierleague': 'Premier League',
            'laliga': 'La Liga',
            'bundesliga': 'Bundesliga', 
            'seriea': 'Serie A',
            'ligue1': 'Ligue 1',
            'championsleague': 'Champions League',
            'europaleague': 'Europa League',
            'worldcup': 'World Cup',
            
            # B√ìNG ƒê√Å QU·ªêC T·∫æ - WEBSITE
            'transfermarkt': 'Chuy·ªÉn nh∆∞·ª£ng',
            '90min': 'B√≥ng ƒë√°',
            'fourfourtwo': 'B√≥ng ƒë√°',
            'squawka': 'B√≥ng ƒë√°',
            'planetfootball': 'B√≥ng ƒë√°',
            'football365': 'B√≥ng ƒë√°',
            'football-italia': 'Serie A',
            'football-espana': 'La Liga',
            'getfootballnewsgermany': 'Bundesliga',
            
            # TH·ªÇ THAO QU·ªêC T·∫æ - T·ªîNG H·ª¢P
            'soccer': 'B√≥ng ƒë√°',
            'football': 'B√≥ng ƒë√°',
            'sportsnews': 'Th·ªÉ thao',
            'sport': 'Th·ªÉ thao',
            'sports': 'Th·ªÉ thao',
            
            # TH·ªÇ THAO QU·ªêC T·∫æ - M√îN TH·ªÇ THAO
            'tennis': 'Tennis',
            'golf': 'Golf',
            'racing': 'ƒêua xe',
            'formula-1': 'Formula 1',
            'f1': 'Formula 1',
            'cricket': 'Cricket',
            'rugby': 'B√≥ng b·∫ßu d·ª•c',
            'boxing': 'Quy·ªÅn anh',
            'cycling': 'ƒêua xe ƒë·∫°p',
            'basketball': 'B√≥ng r·ªï',
            'fiba': 'B√≥ng r·ªï',
            'wtatennis': 'Tennis',
            'atptour': 'Tennis',
            'pgatour': 'Golf',
            'icc-cricket': 'Cricket',
            
            # C√îNG NGH·ªÜ & KHOA H·ªåC
            'tech': 'C√¥ng ngh·ªá',
            'technology': 'C√¥ng ngh·ªá',
            'science': 'Khoa h·ªçc',
            'science_and_environment': 'Khoa h·ªçc',
            'breaking_news': 'Tin m·ªõi nh·∫•t',
            
            # TI·∫æNG ANH - NEWS T·ªîNG H·ª¢P
            'topnews': 'Tin n·ªïi b·∫≠t',
            'worldnews': 'Th·∫ø gi·ªõi',
            'businessnews': 'Kinh doanh',
            'technologynews': 'C√¥ng ngh·ªá',
            'sciencenews': 'Khoa h·ªçc',
            'edition': 'Tin qu·ªëc t·∫ø',
            'edition_world': 'Th·∫ø gi·ªõi',
            'edition_business': 'Kinh doanh',
            'edition_technology': 'C√¥ng ngh·ªá',
            'edition_sport': 'Th·ªÉ thao',
            'international': 'Tin qu·ªëc t·∫ø',
            'world': 'Th·∫ø gi·ªõi',
            'business': 'Kinh doanh',
            'technology': 'C√¥ng ngh·ªá',
            'science': 'Khoa h·ªçc'
        }
        
        for key, value in categories.items():
            if key in feed_url.lower():
                return value
        
        # M·∫∑c ƒë·ªãnh cho c√°c feed kh√¥ng x√°c ƒë·ªãnh
        if 'news' in feed_url.lower():
            return 'Tin qu·ªëc t·∫ø'
        elif 'rss' in feed_url.lower():
            return 'Tin m·ªõi nh·∫•t'
        else:
            return 'Th·∫ø gi·ªõi'

    def _extract_language(self, feed_url):
        """Ph√¢n lo·∫°i ng√¥n ng·ªØ"""
        vietnamese_domains = ['vnexpress', 'dantri', 'thanhnien', 'tuoitre', '24h', 'bongdaplus', 
                            'webthethao', 'thethao247', 'laodong', 'vietnamnet', 'zingnews',
                            'cafef', 'vtv']
        english_domains = ['espn', 'skysports', 'goal', 'eurosport', 'bbc', 'theguardian', 
                        'reuters', 'cnn', 'apnews', 'techcrunch', 'theverge', 'wired',
                        'arstechnica', 'nasa', 'nature', 'premierleague', 'laliga', 'bundesliga',
                        'legaseriea', 'ligue1', 'transfermarkt', '90min', 'fourfourtwo', 'squawka',
                        'planetfootball', 'football365', 'dailymail', 'mirror', 'thesun', 'independent',
                        'fifa', 'uefa', 'nba', 'nfl', 'mlb', 'nhl', 'atptour', 'wtatennis', 'pgatour',
                        'formula1', 'fiba', 'icc-cricket', 'football-italia', 'getfootballnewsgermany',
                        'football-espana', 'science.org', 'newscientist', 'phys.org', 'space.com',
                        'esa.int', 'skyandtelescope', 'sciencedaily', 'livescience', 'sciencenews',
                        'discovermagazine', 'nationalgeographic', 'earthtouchnews', 'worldwildlife',
                        'conservation', 'greenpeace']
        
        if any(domain in feed_url for domain in vietnamese_domains):
            return 'Vietnamese'
        elif any(domain in feed_url for domain in english_domains):
            return 'English'
        else:
            return 'Other'

    def _extract_source(self, feed_url):
        sources = {
            # B√ÅO VI·ªÜT NAM
            'vnexpress': 'VnExpress',
            'dantri': 'D√¢n Tr√≠',
            'thanhnien': 'Thanh Ni√™n',
            'tuoitre': 'Tu·ªïi Tr·∫ª',
            '24h': '24h.com.vn',
            'bongdaplus': 'B√≥ng ƒê√° Plus',
            'webthethao': 'Webthethao',
            'thethao247': 'Th·ªÉ thao 247',
            'laodong': 'Lao ƒê·ªông',
            'vietnamnet': 'VietnamNet',
            'zingnews': 'ZingNews',
            'cafef': 'Cafef',
            'vtv': 'VTV',
            
            # TH·ªÇ THAO QU·ªêC T·∫æ - T·ªîNG H·ª¢P
            'espn': 'ESPN',
            'skysports': 'Sky Sports',
            'goal': 'Goal.com',
            'eurosport': 'Eurosport',
            'reuters': 'Reuters',
            'cnn': 'CNN',
            'bbc': 'BBC',
            'theguardian': 'The Guardian',
            'apnews': 'Associated Press',
            
            # GI·∫¢I ƒê·∫§U B√ìNG ƒê√Å CH√çNH TH·ªêNG
            'premierleague': 'Premier League',
            'laliga': 'La Liga',
            'bundesliga': 'Bundesliga',
            'legaseriea': 'Serie A', 
            'ligue1': 'Ligue 1',
            
            # WEBSITE B√ìNG ƒê√Å
            'transfermarkt': 'Transfermarkt',
            '90min': '90min',
            'fourfourtwo': 'FourFourTwo',
            'squawka': 'Squawka',
            'planetfootball': 'Planet Football',
            'football365': 'Football365',
            'football-italia': 'Football Italia',
            'getfootballnewsgermany': 'Get German Football News',
            'football-espana': 'Football Espana',
            
            # NEWSPAPERS ANH
            'dailymail': 'Daily Mail',
            'mirror': 'Daily Mirror',
            'thesun': 'The Sun',
            'independent': 'The Independent',
            
            # TH·ªÇ THAO CHUY√äN NG√ÄNH
            'atptour': 'ATP Tour',
            'wtatennis': 'WTA Tennis',
            'pgatour': 'PGA Tour',
            'formula1': 'Formula 1',
            'fiba': 'FIBA Basketball',
            'icc-cricket': 'ICC Cricket',
            
            # C√îNG NGH·ªÜ & KHOA H·ªåC
            'techcrunch': 'TechCrunch',
            'theverge': 'The Verge',
            'wired': 'Wired',
            'arstechnica': 'Ars Technica',
            'nasa': 'NASA',
            'nature': 'Nature',

            # TH·∫æ GI·ªöI ƒê·ªòNG V·∫¨T & THI√äN NHI√äN
            'nationalgeographic': 'National Geographic',
            'bbcearth': 'BBC Earth',
            'earthtouchnews': 'Earth Touch News',
            'worldwildlife': 'WWF',
            
            # KHOA H·ªåC
            'science.org': 'Science Magazine',
            'newscientist': 'New Scientist',
            'phys.org': 'Phys.org',
            'sciencedaily': 'Science Daily',
            
            # B·∫¢O T·ªíN & M√îI TR∆Ø·ªúNG
            'conservation': 'Conservation International',
            'greenpeace': 'Greenpeace',
            
            # KHOA H·ªåC V≈® TR·ª§
            'space.com': 'Space.com',
            'nasa': 'NASA',
            'esa.int': 'European Space Agency',
            'skyandtelescope': 'Sky & Telescope',
            
            # KHOA H·ªåC ƒê·ªúI S·ªêNG
            'livescience': 'Live Science',
            'sciencenews': 'Science News',
            'discovermagazine': 'Discover Magazine'
        }
        
        # X·ª≠ l√Ω c√°c domain ph·ª©c t·∫°p
        if 'feeds.reuters.com' in feed_url:
            return 'Reuters'
        elif 'feeds.bbci.co.uk' in feed_url:
            return 'BBC'
        elif 'rss.cnn.com' in feed_url:
            return 'CNN'
        elif 'apf-science' in feed_url:
            return 'Associated Press'
        elif 'science_and_environment' in feed_url:
            return 'BBC Science'
        
        for domain, source in sources.items():
            if domain in feed_url:
                return source
        
        return 'Other'
    
    def save_articles(self, articles, filename='vn_articles.json'):
        filepath = os.path.join(self.data_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        
        print(f"ƒê√£ l∆∞u {len(articles)} b√†i b√°o v√†o: {filepath}")
        
        # T·∫°o file th·ªëng k√™ v·ªõi ph√¢n lo·∫°i m·ªõi
        self._create_statistics_file(articles)
        
        return filepath
    
    def _create_statistics_file(self, articles):
        """T·∫°o file th·ªëng k√™ .txt v·ªõi ph√¢n lo·∫°i theo ch·ªß ƒë·ªÅ v√† ng√¥n ng·ªØ"""
        stats_file = os.path.join(self.data_dir, 'thong_ke_bai_bao.txt')
        
        categories = {}
        languages = {}
        sources = {}
        
        for article in articles:
            cat = article['category']
            lang = article['language']
            src = article['source']
            
            categories[cat] = categories.get(cat, 0) + 1
            languages[lang] = languages.get(lang, 0) + 1
            sources[src] = sources.get(src, 0) + 1
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write("TH·ªêNG K√ä B√ÄI B√ÅO - PH√ÇN LO·∫†I THEO CH·ª¶ ƒê·ªÄ V√Ä NG√îN NG·ªÆ\n")
            f.write("=" * 70 + "\n\n")
            f.write(f"T·ªïng s·ªë b√†i b√°o: {len(articles)}\n")
            f.write(f"Th·ªùi gian th·ªëng k√™: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("PH√ÇN B·ªê THEO CH·ª¶ ƒê·ªÄ:\n")
            f.write("-" * 40 + "\n")
            for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / len(articles)) * 100
                f.write(f"{cat:<25} {count:>4} b√†i ({percentage:5.1f}%)\n")
            
            f.write("\nPH√ÇN B·ªê THEO NG√îN NG·ªÆ:\n")
            f.write("-" * 40 + "\n")
            for lang, count in sorted(languages.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / len(articles)) * 100
                f.write(f"{lang:<15} {count:>4} b√†i ({percentage:5.1f}%)\n")
            
            f.write("\nPH√ÇN B·ªê THEO NGU·ªíN B√ÅO:\n")
            f.write("-" * 40 + "\n")
            for src, count in sorted(sources.items(), key=lambda x: x[1], reverse=True)[:15]:
                percentage = (count / len(articles)) * 100
                f.write(f"{src:<20} {count:>4} b√†i ({percentage:5.1f}%)\n")
        
        print(f"ƒê√£ t·∫°o file th·ªëng k√™: {stats_file}")
    
    def load_articles(self, filename='vn_articles.json'):
        filepath = os.path.join(self.data_dir, filename)
        
        if not os.path.exists(filepath):
            print(f"File {filepath} kh√¥ng t·ªìn t·∫°i!")
            return []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            articles = json.load(f)
        
        print(f"ƒê√£ load {len(articles)} b√†i b√°o t·ª´: {filepath}")
        return articles

def main():
    print("ARTICLE CRAWLER - PH√ÇN LO·∫†I THEO CH·ª¶ ƒê·ªÄ & NG√îN NG·ªÆ")
    print("=" * 60)
    
    crawler = ArticleCrawler()
    
    try:
        max_articles = int(input("S·ªë b√†i b√°o mu·ªën crawl (m·∫∑c ƒë·ªãnh 10000): ").strip() or "10000")
    except:
        max_articles = 10000
    
    articles = crawler.crawl_vnexpress_rss(max_articles=max_articles, verbose=False)
    
    if articles:
        crawler.save_articles(articles)
        
        # Hi·ªÉn th·ªã th·ªëng k√™ nhanh
        categories = {}
        languages = {}
        
        for article in articles:
            cat = article['category']
            lang = article['language']
            categories[cat] = categories.get(cat, 0) + 1
            languages[lang] = languages.get(lang, 0) + 1
        
        print(f"\nTH·ªêNG K√ä NHANH:")
        print(f"T·ªïng s·ªë b√†i b√°o: {len(articles)}")
        print("\nTop ch·ªß ƒë·ªÅ:")
        for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  {cat}: {count} b√†i")
        
        print("\nPh√¢n b·ªë ng√¥n ng·ªØ:")
        for lang, count in sorted(languages.items(), key=lambda x: x[1], reverse=True):
            print(f"  {lang}: {count} b√†i")
        
        print(f"\nCRAWL TH√ÄNH C√îNG!")
        print(f"D·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c: {crawler.data_dir}")
    else:
        print("Kh√¥ng crawl ƒë∆∞·ª£c b√†i b√°o n√†o!")

if __name__ == "__main__":
    main()