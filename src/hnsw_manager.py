import numpy as np
import os
import time
import hnswlib
import json
import pickle
from article_embedder import ArticleEmbedder

class ArticleHNSWManager:
    def __init__(self, index_dir='article_index'):
        self.index_dir = index_dir
        self.dim = 768
        self.index = None
        self.articles = []
        self.embedder = ArticleEmbedder()
        self.all_embeddings = None
        
        os.makedirs(index_dir, exist_ok=True)
    
    def get_index_info(self):
        if self.index is None:
            return {
                'dim': self.dim,
                'article_count': len(self.articles),
                'vector_count': 0,
                'index_dir': self.index_dir
            }
        
        return {
            'dim': self.dim,
            'article_count': len(self.articles),
            'vector_count': self.index.get_current_count(),
            'index_dir': self.index_dir
        }
    
    def build_index(self, articles, max_elements=10000, ef_construction=200, M=16):
        print("ƒêANG X√ÇY D·ª∞NG INDEX T√åM KI·∫æM B√ÄI B√ÅO")
        print("=" * 50)
        
        print(f"T·ªïng s·ªë b√†i b√°o ƒë·∫ßu v√†o: {len(articles)}")
        
        # L·ªçc c√°c b√†i b√°o tr√πng l·∫∑p d·ª±a tr√™n link
        unique_articles = []
        seen_links = set()
        
        for article in articles:
            if article['link'] not in seen_links:
                unique_articles.append(article)
                seen_links.add(article['link'])
        
        print(f"S·ªë b√†i b√°o sau khi l·ªçc tr√πng: {len(unique_articles)}")
        
        valid_articles, embeddings = self.embedder.embed_articles(unique_articles)
        self.articles = valid_articles
        self.all_embeddings = embeddings
        
        if len(embeddings) == 0:
            print("Kh√¥ng c√≥ embeddings ƒë·ªÉ x√¢y d·ª±ng index!")
            return False
        
        print(f"D·ªØ li·ªáu embedding: {len(embeddings)} b√†i b√°o, {embeddings.shape[1]} chi·ªÅu")
        
        # X√¢y d·ª±ng HNSW index
        print("ƒêang x√¢y d·ª±ng HNSW index...")
        start_time = time.time()
        
        self.index = hnswlib.Index(space='cosine', dim=embeddings.shape[1])
        self.index.init_index(max_elements=max_elements, 
                            ef_construction=ef_construction, 
                            M=M)
        
        self.index.add_items(embeddings, np.arange(len(embeddings)))
        build_time = time.time() - start_time
        
        print(f"Th·ªùi gian x√¢y d·ª±ng HNSW: {build_time:.4f}s")
        
        # L∆∞u metadata v√† index
        self._save_metadata()
        index_path = os.path.join(self.index_dir, 'article_index.bin')
        self.index.save_index(index_path)
        
        print("X√ÇY D·ª∞NG INDEX HO√ÄN T·∫§T!")
        return True
    
    def _cosine_similarity(self, vec1, vec2):
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return dot_product / (norm1 * norm2)
    
    def _save_metadata(self):
        metadata = {
            'dim': self.dim,
            'total_articles': len(self.articles),
            'articles': self.articles,
            'build_time': time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        
        metadata_path = os.path.join(self.index_dir, 'metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # L∆∞u embeddings ri√™ng ƒë·ªÉ tr√°nh file qu√° l·ªõn
        if self.all_embeddings is not None:
            embeddings_path = os.path.join(self.index_dir, 'embeddings.npy')
            np.save(embeddings_path, self.all_embeddings)
            print(f"ƒê√£ l∆∞u embeddings: {embeddings_path}")
    
    def load_index(self):
        print(f"ƒêang t·∫£i index t·ª´ {self.index_dir}...")
        
        metadata_path = os.path.join(self.index_dir, 'metadata.json')
        if not os.path.exists(metadata_path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y metadata: {metadata_path}")
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        self.dim = metadata['dim']
        self.articles = metadata['articles']
        
        # T·∫£i embeddings t·ª´ file .npy
        embeddings_path = os.path.join(self.index_dir, 'embeddings.npy')
        if os.path.exists(embeddings_path):
            print("ƒêang t·∫£i embeddings t·ª´ file...")
            self.all_embeddings = np.load(embeddings_path)
            print(f"ƒê√£ t·∫£i {len(self.all_embeddings)} embeddings")
        else:
            print("Kh√¥ng t√¨m th·∫•y embeddings cache, c·∫ßn embed l·∫°i...")
            valid_articles, embeddings = self.embedder.embed_articles(self.articles)
            self.all_embeddings = embeddings
        
        index_path = os.path.join(self.index_dir, 'article_index.bin')
        if not os.path.exists(index_path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file index: {index_path}")
        
        self.index = hnswlib.Index(space='cosine', dim=self.dim)
        self.index.load_index(index_path)
        self.index.set_ef(100)
        
        print(f"T·∫£i th√†nh c√¥ng: {len(self.articles)} b√†i b√°o")
        return True
    
    def search_with_comparison(self, query, k=10):
        if self.index is None or self.all_embeddings is None:
            raise RuntimeError("H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o!")
        
        print(f"SO S√ÅNH T√åM KI·∫æM: '{query}'")
        print("=" * 60)
        
        query_vector = self.embedder.embed_query(query)
        
        # Brute Force
        print("BRUTE FORCE...")
        start_time = time.time()
        
        similarities = []
        for i in range(len(self.all_embeddings)):
            similarity = self._cosine_similarity(query_vector[0], self.all_embeddings[i])
            similarities.append((i, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        brute_results = similarities[:k]
        brute_time = time.time() - start_time
        
        # HNSW
        print("HNSW SEARCH...")
        start_time = time.time()
        
        labels, distances = self.index.knn_query(query_vector, k=k)
        hnsw_time = time.time() - start_time
        
        hnsw_results = []
        for i, (label, distance) in enumerate(zip(labels[0], distances[0])):
            similarity = 1 - distance
            hnsw_results.append((int(label), similarity))
        
        # So s√°nh
        print(f"K·∫æT QU·∫¢ SO S√ÅNH:")
        print(f"  Brute Force: {brute_time:.4f}s")
        print(f"  HNSW: {hnsw_time:.4f}s")
        print(f"  T·ªëc ƒë·ªô tƒÉng: {brute_time/hnsw_time:.1f}x")
        
        brute_indices = {idx for idx, _ in brute_results}
        hnsw_indices = {idx for idx, _ in hnsw_results}
        common_results = brute_indices & hnsw_indices
        accuracy = len(common_results) / k
        
        print(f"  ƒê·ªô ch√≠nh x√°c Top-{k}: {accuracy:.1%}")
        print(f"  K·∫øt qu·∫£ tr√πng: {len(common_results)}/{k}")
        
        return {
            'query': query,
            'brute_force': {
                'results': [{'index': idx, 'similarity': sim} for idx, sim in brute_results],
                'time': brute_time
            },
            'hnsw': {
                'results': [{'index': idx, 'similarity': sim} for idx, sim in hnsw_results],
                'time': hnsw_time
            },
            'comparison': {
                'speedup': brute_time / hnsw_time,
                'accuracy': accuracy,
                'common_results': len(common_results)
            }
        }
    
    def display_search_results(self, search_result, show_details=True):
        """Hi·ªÉn th·ªã k·∫øt qu·∫£ t√¨m ki·∫øm"""
        query = search_result['query']
        hnsw_results = search_result['hnsw']['results']
        
        print(f"\nTOP K·∫æT QU·∫¢ CHO: '{query}'")
        print("-" * 80)
        
        for i, result in enumerate(hnsw_results[:5], 1):
            idx = result['index']
            similarity = result['similarity']
            article = self.articles[idx]
            
            print(f"{i}. [{similarity:.3f}] {article['title']}")
            print(f"   üìç {article['source']} | {article['category']} | {article['language']}")
            if show_details and article['summary']:
                summary = article['summary'][:150] + "..." if len(article['summary']) > 150 else article['summary']
                print(f"   üìù {summary}")
            print()
    
    def benchmark_multiple_queries(self, queries, k=10):
        print(f"BENCHMARK V·ªöI {len(queries)} QUERIES")
        print("=" * 60)
        
        results = []
        for query in queries:
            comparison = self.search_with_comparison(query, k=k)
            results.append(comparison)
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£ cho query n√†y
            self.display_search_results(comparison, show_details=True)
        
        # T·ªïng k·∫øt benchmark
        print(f"\n{'='*80}")
        print("T·ªîNG K·∫æT BENCHMARK")
        print(f"{'='*80}")
        
        avg_speedup = np.mean([r['comparison']['speedup'] for r in results])
        avg_accuracy = np.mean([r['comparison']['accuracy'] for r in results])
        avg_brute_time = np.mean([r['brute_force']['time'] for r in results])
        avg_hnsw_time = np.mean([r['hnsw']['time'] for r in results])
        
        print(f"Th·ªùi gian Brute Force trung b√¨nh: {avg_brute_time:.4f}s")
        print(f"Th·ªùi gian HNSW trung b√¨nh: {avg_hnsw_time:.4f}s")
        print(f"T·ªëc ƒë·ªô tƒÉng trung b√¨nh: {avg_speedup:.1f}x")
        print(f"ƒê·ªô ch√≠nh x√°c trung b√¨nh: {avg_accuracy:.1%}")
        
        # Hi·ªÉn th·ªã chi ti·∫øt t·ª´ng query
        print(f"\n{'='*80}")
        print("CHI TI·∫æT T·ª™NG QUERY")
        print(f"{'='*80}")
        
        for result in results:
            query = result['query']
            speedup = result['comparison']['speedup']
            accuracy = result['comparison']['accuracy']
            brute_time = result['brute_force']['time']
            hnsw_time = result['hnsw']['time']
            
            print(f"'{query}': Brute={brute_time:.4f}s, HNSW={hnsw_time:.4f}s, "
                  f"Speedup={speedup:.1f}x, Accuracy={accuracy:.1%}")
        
        return results

def build_and_test_article_index():
    from crawl_articles import ArticleCrawler
    
    print("BENCHMARK: HNSW vs BRUTE FORCE")
    print("=" * 50)
    
    # T·∫£i ho·∫∑c crawl d·ªØ li·ªáu m·ªõi
    crawler = ArticleCrawler()
    articles = crawler.load_articles()
    
    if not articles:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu b√†i b√°o, ƒëang crawl m·ªõi...")
        articles = crawler.crawl_vnexpress_rss(max_articles=1000, verbose=False)
        if articles:
            crawler.save_articles(articles)
        else:
            print("Kh√¥ng crawl ƒë∆∞·ª£c d·ªØ li·ªáu m·ªõi!")
            return
    
    print(f"ƒê√£ t·∫£i {len(articles)} b√†i b√°o")
    
    # X√¢y d·ª±ng index
    hnsw_mgr = ArticleHNSWManager()
    success = hnsw_mgr.build_index(articles)
    
    if not success:
        print("X√¢y d·ª±ng index th·∫•t b·∫°i!")
        return
    
    # Test v·ªõi queries ƒëa d·∫°ng
    test_queries = [
        "b√≥ng ƒë√° Premier League",          # Th·ªÉ thao qu·ªëc t·∫ø
        "ch·ª©ng kho√°n th·ªã tr∆∞·ªùng",          # Kinh t·∫ø
        "c√¥ng ngh·ªá AI tr√≠ tu·ªá nh√¢n t·∫°o",   # C√¥ng ngh·ªá
        "s·ª©c kh·ªèe dinh d∆∞·ª°ng",             # S·ª©c kh·ªèe
        "gi√°o d·ª•c ƒë·∫°i h·ªçc",                # Gi√°o d·ª•c
        "phim ·∫£nh Hollywood",              # Gi·∫£i tr√≠
        "du l·ªãch Ch√¢u √Çu",                 # Du l·ªãch
        "bi·∫øn ƒë·ªïi kh√≠ h·∫≠u",                # Khoa h·ªçc
        "lu·∫≠t ph√°p h√¨nh s·ª±"                # Ph√°p lu·∫≠t
    ]
    
    print(f"\nB·∫ÆT ƒê·∫¶U TEST V·ªöI {len(test_queries)} QUERIES ƒêA D·∫†NG")
    print("=" * 80)
    
    benchmark_results = hnsw_mgr.benchmark_multiple_queries(test_queries, k=10)
    
    # L∆∞u k·∫øt qu·∫£ benchmark
    benchmark_file = os.path.join(hnsw_mgr.index_dir, 'benchmark_results.json')
    with open(benchmark_file, 'w', encoding='utf-8') as f:
        json.dump(benchmark_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nƒê√£ l∆∞u k·∫øt qu·∫£ benchmark: {benchmark_file}")
    print("BENCHMARK HO√ÄN T·∫§T!")

def test_existing_index():
    """Test v·ªõi index ƒë√£ c√≥ s·∫µn"""
    hnsw_mgr = ArticleHNSWManager()
    
    try:
        hnsw_mgr.load_index()
        print(f"ƒê√£ t·∫£i index v·ªõi {len(hnsw_mgr.articles)} b√†i b√°o")
        
        # Test nhanh
        test_queries = ["b√≥ng ƒë√°", "c√¥ng ngh·ªá", "ch√≠nh tr·ªã"]
        hnsw_mgr.benchmark_multiple_queries(test_queries, k=5)
        
    except Exception as e:
        print(f"L·ªói khi t·∫£i index: {e}")
        print("C·∫ßn x√¢y d·ª±ng index m·ªõi...")
        build_and_test_article_index()

if __name__ == "__main__":
    print("HNSW MANAGER - H·ªÜ TH·ªêNG T√åM KI·∫æM B√ÄI B√ÅO")
    print("=" * 50)
    
    # Ki·ªÉm tra xem index ƒë√£ t·ªìn t·∫°i ch∆∞a
    index_exists = os.path.exists(os.path.join('article_index', 'metadata.json'))
    
    if index_exists:
        choice = input("Index ƒë√£ t·ªìn t·∫°i. B·∫°n mu·ªën:\n1. Test index hi·ªán c√≥\n2. X√¢y d·ª±ng index m·ªõi\nCh·ªçn (1/2): ").strip()
        if choice == "1":
            test_existing_index()
        else:
            build_and_test_article_index()
    else:
        build_and_test_article_index()